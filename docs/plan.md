

## **Implementation Plan: Direct-Control Figma Agent**

This document outlines the phased implementation of a direct-control Figma agent. The goal is to replace the current MCP-based architecture with a simplified, high-performance system where a Python AI agent directly controls the Figma plugin via a WebSocket bridge.

The plan is structured as a series of iterative, testable milestones. Each phase builds upon the last, ensuring a stable foundation and clear progress.



#### **Core Agent Capability: Resilient & Self-Correcting Tool Use**

Before implementing the specific tool, we will build a robust, multi-layered error-handling and self-correction architecture. This is a foundational capability required for a reliable agent and prevents the agent from getting stuck in failure loops. This system consists of three layers:

**Layer 1: The Programmatic Supervisor (`tool_use_behavior`)**
This is the primary mechanism for intelligent failure recovery, built using a custom function for the `tool_use_behavior` parameter in the `Agent` definition. It acts as a supervisor that intercepts and processes tool results before the LLM sees them.

*   **Logic:**
    1.  **Intercepts** the result of every tool call.
    2.  **Detects Failures** by checking if the result is a `ToolExecutionError`.
    3.  **Injects a "System Alert"** into the conversation history if a failure is detected. This alert is a clean, structured message explaining the failure and instructing the agent to re-plan. For example: `System Alert: The tool 'create_text' failed. Reason: 'Cannot add elements to a shape node.' My previous plan was flawed. I must now create a container Frame first.`
    4.  **Hides Raw Error** from the LLM, preventing confusion and ensuring the agent receives a clear, actionable signal.

**Layer 2: Smarter Instructions (System Prompt)**
The agent's system prompt will be updated to teach it how to interpret and act upon the "System Alerts" generated by the supervisor. This creates a powerful synergy between programmatic control and LLM reasoning.

*   **New Instructions:**
    > **"### Tool Failure and Self-Correction Protocol**
    >
    > If a tool fails, a 'System Alert' will be added to our conversation. You MUST stop your current plan and create a new one based on the alert's reason.
    >
    > **- If the alert mentions 'Cannot add elements' or 'node does not support children':** Your new plan is to first create a `frame` to serve as a container. Then, re-attempt to create your element inside that new frame.
    > **- If the alert mentions a 'node not found':** Your new plan is to use tools like `get_selection` to re-assess the canvas before proceeding. The element may have been deleted.

**Layer 3: The "Circuit Breaker" Guardrail**
This is a final safety net to prevent any runaway loops that might slip past the first two layers.

*   **Logic:**
    1.  Implement an `InputGuardrail` that runs before the agent.
    2.  The guardrail inspects the conversation history to detect if the agent is about to retry the *exact same failed tool call* from the previous turn.
    3.  If this repetitive failure pattern is detected, the guardrail **stops the agent's run** and returns a clear, final message to the user, preventing a `MaxTurnsExceeded` crash and improving the user experience.

By implementing this three-layered architecture, the agent's reliability will be dramatically increased, forming a solid foundation for all future tool development.

--

*   **Streaming & Real-time Feedback:** Stream the agent's thoughts and actions to the plugin UI for a more interactive experience.
*   **Advanced Tools:** Build more complex composite tools (e.g., `create_login_form`, `generate_color_palette`).
*   **Error Handling & Resilience:** Enhance error handling in the Python tools, including retries and more descriptive feedback to the agent.
*   **State Awareness:** Implement tools for reading Figma state (`get_selection`, `get_node_properties`) so the agent can perform contextual actions.